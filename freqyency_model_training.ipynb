{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers, models, optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_size = (128,128,1), verbose = False):\n",
    "    #size filter input\n",
    "    size_filter_in = 16\n",
    "    #normal initialization of weights\n",
    "    kernel_init = 'he_normal'\n",
    "    #To apply leaky relu after the conv layer \n",
    "    activation_layer = None\n",
    "    inputs = layers.Input(input_size)\n",
    "    conv1 = layers.Conv2D(size_filter_in, \n",
    "                          3, \n",
    "                          activation = activation_layer, \n",
    "                          padding = 'same', \n",
    "                          kernel_initializer = kernel_init)(inputs)\n",
    "    conv1 = layers.LeakyReLU()(conv1)\n",
    "    conv1 = layers.Conv2D(size_filter_in, \n",
    "                          3, \n",
    "                          activation = activation_layer, \n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(conv1)\n",
    "    conv1 = layers.LeakyReLU()(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = layers.Conv2D(size_filter_in * 2, \n",
    "                          3, \n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same', \n",
    "                          kernel_initializer = kernel_init)(pool1)\n",
    "    conv2 = layers.LeakyReLU()(conv2)\n",
    "    conv2 = layers.Conv2D(size_filter_in * 2, \n",
    "                          3, \n",
    "                          activation = activation_layer, \n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(conv2)\n",
    "    conv2 = layers.LeakyReLU()(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = layers.Conv2D(size_filter_in * 4, \n",
    "                          3, \n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(pool2)\n",
    "    conv3 = layers.LeakyReLU()(conv3)\n",
    "    conv3 = layers.Conv2D(size_filter_in * 4, \n",
    "                          3,\n",
    "                          activation = activation_layer, \n",
    "                          padding = 'same', \n",
    "                          kernel_initializer = kernel_init)(conv3)\n",
    "    conv3 = layers.LeakyReLU()(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = layers.Conv2D(size_filter_in * 8, \n",
    "                          3, \n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same', \n",
    "                          kernel_initializer = kernel_init)(pool3)\n",
    "    conv4 = layers.LeakyReLU()(conv4)\n",
    "    conv4 = layers.Conv2D(size_filter_in * 8,\n",
    "                          3,\n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(conv4)\n",
    "    conv4 = layers.LeakyReLU()(conv4)\n",
    "    drop4 = layers.Dropout(0.5)(conv4)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = layers.Conv2D(size_filter_in * 16,\n",
    "                          3, activation = activation_layer,\n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(pool4)\n",
    "    conv5 = layers.LeakyReLU()(conv5)\n",
    "    conv5 = layers.Conv2D(size_filter_in * 16, \n",
    "                          3,\n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(conv5)\n",
    "    conv5 = layers.LeakyReLU()(conv5)\n",
    "    drop5 = layers.Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = layers.Conv2D(size_filter_in * 8,\n",
    "                        2,\n",
    "                        activation = activation_layer,\n",
    "                        padding = 'same',\n",
    "                        kernel_initializer = kernel_init)(layers.UpSampling2D(size = (2,2))(drop5))\n",
    "    up6 = layers.LeakyReLU()(up6)\n",
    "\n",
    "    merge6 = layers.Concatenate(axis = 3)([drop4, up6])\n",
    "    conv6 = layers.Conv2D(size_filter_in * 8,\n",
    "                          3,\n",
    "                          activation = activation_layer, \n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(merge6)\n",
    "    conv6 = layers.LeakyReLU()(conv6)\n",
    "    conv6 = layers.Conv2D(size_filter_in * 8, \n",
    "                          3, \n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same', \n",
    "                          kernel_initializer = kernel_init)(conv6)\n",
    "    conv6 = layers.LeakyReLU()(conv6)\n",
    "    up7 = layers.Conv2D(size_filter_in * 4,\n",
    "                        2,\n",
    "                        activation = activation_layer,\n",
    "                        padding = 'same',\n",
    "                        kernel_initializer = kernel_init)(layers.UpSampling2D(size = (2,2))(conv6))\n",
    "    up7 = layers.LeakyReLU()(up7)\n",
    "\n",
    "    merge7 = layers.Concatenate(axis = 3)([conv3, up7])\n",
    "    conv7 = layers.Conv2D(size_filter_in * 4, \n",
    "                          3, \n",
    "                          activation = activation_layer, \n",
    "                          padding = 'same', \n",
    "                          kernel_initializer = kernel_init)(merge7)\n",
    "    conv7 = layers.LeakyReLU()(conv7)\n",
    "    conv7 = layers.Conv2D(size_filter_in * 4, \n",
    "                          3,\n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(conv7)\n",
    "    conv7 = layers.LeakyReLU()(conv7)\n",
    "    up8 = layers.Conv2D(size_filter_in * 2,\n",
    "                        2,\n",
    "                        activation = activation_layer, \n",
    "                        padding = 'same',\n",
    "                        kernel_initializer = kernel_init)(layers.UpSampling2D(size = (2,2))(conv7))\n",
    "    up8 = layers.LeakyReLU()(up8)\n",
    "\n",
    "    merge8 = layers.Concatenate(axis = 3)([conv2, up8])\n",
    "    conv8 = layers.Conv2D(size_filter_in * 2,\n",
    "                          3,\n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(merge8)\n",
    "    conv8 = layers.LeakyReLU()(conv8)\n",
    "    conv8 = layers.Conv2D(size_filter_in * 2,\n",
    "                          3,\n",
    "                          activation = activation_layer, \n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(conv8)\n",
    "    conv8 = layers.LeakyReLU()(conv8)\n",
    "\n",
    "    up9 = layers.Conv2D(size_filter_in,\n",
    "                        2,\n",
    "                        activation = activation_layer,\n",
    "                        padding = 'same',\n",
    "                        kernel_initializer = kernel_init)(layers.UpSampling2D(size = (2,2))(conv8))\n",
    "    up9 = layers.LeakyReLU()(up9)\n",
    "\n",
    "    merge9 = layers.Concatenate(axis = 3)([conv1, up9])\n",
    "    conv9 = layers.Conv2D(size_filter_in,\n",
    "                          3, \n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(merge9)\n",
    "    conv9 = layers.LeakyReLU()(conv9)\n",
    "    conv9 = layers.Conv2D(size_filter_in,\n",
    "                          3,\n",
    "                          activation = activation_layer, \n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(conv9)\n",
    "    conv9 = layers.LeakyReLU()(conv9)\n",
    "    conv9 = layers.Conv2D(2, \n",
    "                          3,\n",
    "                          activation = activation_layer,\n",
    "                          padding = 'same',\n",
    "                          kernel_initializer = kernel_init)(conv9)\n",
    "    conv9 = layers.LeakyReLU()(conv9)\n",
    "    conv10 = layers.Conv2D(1, 1, activation = 'tanh')(conv9)\n",
    "\n",
    "    model = models.Model(inputs,conv10)\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "model = unet(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import random\n",
    "from keras.utils.data_utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 255\n",
    "hop_length_fft = 64\n",
    "dim_square_spec = int(n_fft / 2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(Sequence):\n",
    "    def __init__(self,\n",
    "                clean_audio_path,\n",
    "                noise_audio_path,\n",
    "                batch_size = 5,\n",
    "                file_size = 100,\n",
    "                wave_size = 8192):\n",
    "        self.clean_audio_path = clean_audio_path\n",
    "        self.noise_audio_path = noise_audio_path\n",
    "        self.batch_size = batch_size\n",
    "        self.file_size = file_size\n",
    "        self.wave_size = wave_size\n",
    "        \n",
    "        self.n_fft = 255\n",
    "        self.hop_length = 64\n",
    "        self.dim_square_spec = int(self.n_fft / 2) + 1\n",
    "        \n",
    "        self.clean_file_list = self._load_audio_list(self.clean_audio_path)\n",
    "        self.noise_file_list = self._load_audio_list(self.noise_audio_path)\n",
    "        \n",
    "        if isinstance(self.file_size, int):\n",
    "            self.clean_file_list = self.clean_file_list[:self.file_size]\n",
    "            self.noise_file_list = self.noise_file_list[:self.file_size]\n",
    "        elif isinstance(self.file_size, float):\n",
    "            n = int(self.file_size * len(self.clean_file_list))\n",
    "            self.clean_file_list = self.clean_file_list[:n]\n",
    "            self.noise_file_list = self.noise_file_list[:n]\n",
    "        \n",
    "    def _load_audio_list(self, path):\n",
    "        assert os.path.exists(path), f\"{path} not exists.\"\n",
    "        return [os.path.join(path, file) for file in os.listdir(path) if file != \".DS_Store\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.clean_file_list) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.batch_size == 1:\n",
    "            x, y = self._batch_1(index)\n",
    "            return np.expand_dims(x, 0), np.expand_dims(y, 0)\n",
    "        else:\n",
    "            beg = index * self.batch_size\n",
    "            end = (index + 1) * self.batch_size\n",
    "            return self._batch_n(beg, end)\n",
    "    \n",
    "    def _batch_1(self, index):\n",
    "        x, y = self._get_audio_wave(index)\n",
    "        \n",
    "        y = x - y\n",
    "        x_magnitude, x_phase = self._wave_to_magnitude_db_and_phase(x, n_fft = self.n_fft, hop_length = self.hop_length)\n",
    "        y_magnitude, y_phase = self._wave_to_magnitude_db_and_phase(y, n_fft = self.n_fft, hop_length = self.hop_length)\n",
    "        \n",
    "#         y_magnitude = x_magnitude - y_magnitude\n",
    "        x_magnitude = self._normalize(x_magnitude)\n",
    "        y_magnitude = self._normalize(y_magnitude)\n",
    "        return np.expand_dims(x_magnitude, -1), np.expand_dims(y_magnitude, -1)\n",
    "    \n",
    "    def _batch_n(self, beg, end):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(beg, end):\n",
    "            x, y = self._batch_1(i)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    def _load_audio(self, path):\n",
    "        wave, sr = librosa.load(path, mono = True, sr = None)\n",
    "        return wave\n",
    "    \n",
    "    def _get_audio_wave(self, index):\n",
    "        clean_file = self.clean_file_list[index]\n",
    "        noise_file = os.path.join(self.noise_audio_path, clean_file.rsplit(\"/\", 1)[1])\n",
    "        clean_wave = self._load_audio(clean_file)\n",
    "        noise_wave = self._load_audio(noise_file)\n",
    "        start_location = np.random.randint(len(clean_wave) - self.wave_size)\n",
    "        return noise_wave[start_location: start_location + self.wave_size], \\\n",
    "                clean_wave[start_location: start_location + self.wave_size]\n",
    "    \n",
    "    def _wave_to_magnitude_db_and_phase(self, wave, n_fft, hop_length):\n",
    "        stftaudio = librosa.stft(wave, n_fft=n_fft, hop_length=hop_length)\n",
    "        stftaudio_magnitude, stftaudio_phase = librosa.magphase(stftaudio)\n",
    "        stftaudio_magnitude_db = librosa.amplitude_to_db(stftaudio_magnitude, ref=np.max)\n",
    "        return stftaudio_magnitude_db, stftaudio_phase\n",
    "        \n",
    "    def _normalize(self, x):\n",
    "        min_val = np.min(x)\n",
    "        max_val = np.max(x)\n",
    "        x = (x - min_val) / ((max_val - min_val) / 2)\n",
    "        return x - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 128, 128, 1)\n",
      "(5, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "clean_audio_path = \"/Volumes/IPEVO_X0244/speech_to_text/speech_to_text_dataset/clean_trainset_56spk_wav/\"\n",
    "noise_audio_path = \"/Volumes/IPEVO_X0244/speech_to_text/speech_to_text_dataset/noisy_trainset_56spk_wav/\"\n",
    "\n",
    "g = data_generator(clean_audio_path = clean_audio_path, noise_audio_path= noise_audio_path, file_size= 0.5)\n",
    "\n",
    "d = g.__getitem__(0)\n",
    "print(d[0].shape)\n",
    "print(d[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2307/2307 [==============================] - 1095s 475ms/step - loss: 0.0233\n",
      "Epoch 2/10\n",
      "2307/2307 [==============================] - 1099s 476ms/step - loss: 0.0166\n",
      "Epoch 3/10\n",
      "2307/2307 [==============================] - 1097s 476ms/step - loss: 0.0142\n",
      "Epoch 4/10\n",
      "2307/2307 [==============================] - 1148s 497ms/step - loss: 0.0128\n",
      "Epoch 5/10\n",
      " 755/2307 [========>.....................] - ETA: 15:15 - loss: 0.0128"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-5e778e0a2faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuber_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/speech/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/speech/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/speech/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/speech/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/speech/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/speech/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = unet(verbose = False)\n",
    "model.compile(optimizer= keras.optimizers.adam(learning_rate= 1e-4),loss = keras.losses.huber_loss)\n",
    "model.fit_generator(g, epochs=10,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
